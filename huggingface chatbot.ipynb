{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import langchain\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain.cache import SQLiteCache\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "langchain.llm_cache = SQLiteCache(database_path=\".langchain.db\")\n",
    "\n",
    "# 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\n",
    "from getpass import getpass\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass()\n",
    "\n",
    "from llama_index import (\n",
    "    GPTKeywordTableIndex,\n",
    "    GPTVectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    LLMPredictor,\n",
    "    ServiceContext\n",
    ")\n",
    "from langchain import OpenAI\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, query_engine):\n",
    "        self.query_engine = query_engine\n",
    "        self.chat_history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        prompt = \"\\n\".join([f\"{message['role']}: {message['content']}\" for message in self.chat_history[-5:]])\n",
    "        prompt += f\"\\nUser: {user_input}\"\n",
    "        response = self.query_engine.query(user_input)\n",
    "\n",
    "        message = {\"role\": \"assistant\", \"content\": response.response}\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.chat_history.append(message)\n",
    "        return response.response\n",
    "    \n",
    "    def load_chat_history(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                self.chat_history = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    def save_chat_history(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.chat_history, f)\n",
    "\n",
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "\n",
    "# define LLM\n",
    "#flanllm_predictor = LLMPredictor(llm=\n",
    "# HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":.5, \"max_length\":64}, \n",
    "#   huggingfacehub_api_token=\"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\"))\n",
    "\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\" \n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "from llama_index.llm_predictor import HuggingFaceLLMPredictor\n",
    "import torch\n",
    "stablelm_predictor = HuggingFaceLLMPredictor(\n",
    "    max_input_size=4096, \n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    do_sample=False,\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(chunk_size_limit=1024, llm_predictor=stablelm_predictor)\n",
    "\n",
    "# rebuild storage context\n",
    "# from llama_index import StorageContext, load_index_from_storage\n",
    "# storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "# load index\n",
    "# index = load_index_from_storage(storage_context)\n",
    "\n",
    "# build index (first time)\n",
    "# index = GPTKeywordTableIndex.from_documents(documents, service_context=service_context)\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "index.storage_context.persist()\n",
    "query_engine = index.as_query_engine(streaming=True)\n",
    "\n",
    "# get response from query\n",
    "# response = query_engine.query(\"What did the author do after his time at Y Combinator?\")\n",
    "\n",
    "bot = Chatbot(query_engine=query_engine)\n",
    "bot.load_chat_history(\"chat_history.json\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"bye\", \"goodbye\"]:\n",
    "        print(\"Bot: Goodbye!\")\n",
    "        bot.save_chat_history(\"chat_history.json\")\n",
    "        break\n",
    "    streaming_resp = bot.generate_response(user_input)\n",
    "    print(f\"Bot: {streaming_resp.print_response_stream()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

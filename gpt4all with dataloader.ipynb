{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SetList api saved to file\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from setlist_app import SetListApp\n",
    "\n",
    "app = SetListApp(\"tmG3-KHsciHD1mS5y58b1FIv5NMPccWTKN8E\")\n",
    "\n",
    "for iter in range(10, 11): \n",
    "    r = await app.get_setList(artistName=\"Taylor Swift\", p=iter)\n",
    "    data = await r.text()\n",
    "    fileName = \"./data/get_data\" + str(iter) + \".json\"\n",
    "    with open(fileName, \"w\") as f:\n",
    "        # Write the JSON data to the file\n",
    "        f.write(data)\n",
    "#print(f\"{await r.text()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create document loader(s)\n",
    "\n",
    "from langchain.document_loaders import JSONLoader\n",
    "from llama_index import Document\n",
    "import datetime, re\n",
    "from pprint import pprint \n",
    "\n",
    "import unicodedata\n",
    "def remove_nonascii(s: str):\n",
    "    return str(unicodedata.normalize(\"NFKD\", s)).encode('ascii','ignore').decode('ascii')\n",
    "\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"event date\"] = remove_nonascii(record[\"eventDate\"])\n",
    "    metadata[\"venue name\"] = remove_nonascii(record[\"venue\"].get(\"name\"))\n",
    "    metadata[\"city\"] = remove_nonascii(record[\"venue\"].get(\"city\").get(\"name\"))\n",
    "    metadata[\"tour name\"] = remove_nonascii(record[\"tour\"].get(\"name\"))\n",
    "    metadata[\"artist name\"] = remove_nonascii(record[\"artist\"].get(\"name\"))\n",
    "\n",
    "    songs = []    \n",
    "    for e in record[\"sets\"].get(\"set\"):\n",
    "        for s in e.get(\"song\"):\n",
    "            if (s.get(\"name\")):\n",
    "                songs.append(remove_nonascii(s.get(\"name\")))\n",
    "    \n",
    "    metadata[\"songs played\"] = \", \".join(songs)       \n",
    "    return metadata\n",
    "\n",
    "loader = JSONLoader(file_path='./data/get_data1.json', \n",
    "                    jq_schema='.setlist[]',\n",
    "                    content_key='sets',\n",
    "                    text_content=False,\n",
    "                    metadata_func=metadata_func)\n",
    "lcDocuments = loader.load()\n",
    "#pprint(lcDocuments)\n",
    "\n",
    "lDocs = []\n",
    "for doc in lcDocuments:\n",
    "    d = Document.from_langchain_format(doc)\n",
    "    dArray = re.split('-', doc.metadata[\"event date\"])\n",
    "    formattedDate =  datetime.date(int(dArray[2]), int(dArray[1]), int(dArray[0])).strftime('%B %d, %Y')    \n",
    "    template = \"\"\"On {date}, the artist {artist} played in {city} at the venue {venue} to support the tour '{tour}'. \\\n",
    "The artist played the following songs during the concert:  {setlist}\"\"\"\n",
    "    d.text = template.format(date=formattedDate, artist=doc.metadata[\"artist name\"], city=doc.metadata[\"city\"], venue=doc.metadata[\"venue name\"], \n",
    "                             tour=doc.metadata[\"tour name\"], setlist=doc.metadata[\"songs played\"])\n",
    "    lDocs.append(d)\n",
    "\n",
    "pprint(lDocs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index using loader\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging, sys\n",
    "\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from llama_index import (GPTVectorStoreIndex,LLMPredictor,ServiceContext)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "embed_model = LangchainEmbedding(HuggingFaceInstructEmbeddings(model_kwargs = {'device': 'cpu'}, model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
    "model_path = '../models/ggml-gpt4all-j-v1.3-groovy.bin'\n",
    "llm = GPT4All(model=model_path, backend='gptj', callbacks=[StreamingStdOutCallbackHandler()], n_batch=64, streaming=False, n_ctx=64, n_threads=8, verbose=True)\n",
    "gpt4all_lm_predictor = LLMPredictor(llm=llm)\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=gpt4all_lm_predictor,\n",
    "    embed_model=embed_model,\n",
    "    node_parser=SimpleNodeParser(text_splitter=TokenTextSplitter(chunk_size=512, chunk_overlap=32))\n",
    ")\n",
    "index = GPTVectorStoreIndex.from_documents(lDocs, service_context=service_context)\n",
    "index.storage_context.persist()\n",
    "\n",
    "#query_engine = index.as_query_engine(streaming=True, similarity_top_k=1, service_context=service_context)\n",
    "#response_stream = query_engine.query(\"How many times was 'Cruel Summer' played in a set list?\")\n",
    "#response_stream.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use cached index to run queries\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain import PromptTemplate\n",
    "from langchain.embeddings.huggingface import HuggingFaceInstructEmbeddings\n",
    "import logging, sys\n",
    "\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.langchain_helpers.text_splitter import TokenTextSplitter\n",
    "from langchain.cache import SQLiteCache\n",
    "from llama_index import (\n",
    "    GPTVectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    "    LLMPredictor,\n",
    "    PromptHelper,\n",
    "    StorageContext,\n",
    "    ServiceContext\n",
    ")\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Swifty chat bot\",\n",
    "        func=lambda q: query_engine.query(q),\n",
    "        description=f\"Useful when you want answer questions about the set list Documents.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "embed_model = LangchainEmbedding(HuggingFaceInstructEmbeddings(model_kwargs = {'device': 'cpu'}, model_name=\"sentence-transformers/all-mpnet-base-v2\"))\n",
    "model_path = '../models/ggml-gpt4all-j-v1.3-groovy.bin'\n",
    "gpt4all_lm_predictor = LLMPredictor(\n",
    "    cache=SQLiteCache(database_path=\".langchain.db\"),\n",
    "    llm=GPT4All(model=model_path, backend='gptj', temp=0.8, n_batch=24, callbacks=[StreamingStdOutCallbackHandler()], streaming=False, n_ctx=1024, n_threads=4, verbose=False))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=gpt4all_lm_predictor, embed_model=embed_model,\n",
    "    node_parser=SimpleNodeParser(text_splitter=TokenTextSplitter(chunk_size=512, chunk_overlap=32)))\n",
    "index = load_index_from_storage(StorageContext.from_defaults(persist_dir=\"./storage\"), service_context=service_context) \n",
    "query_engine = index.as_query_engine(streaming=False, similarity_top_k=2, service_context=service_context, verbose=False)\n",
    "#print(query_engine.query(\"Where did Taylor Swift play most recently?\"))\n",
    "print(query_engine.query(\"What songs did Taylor Swift play in Atlanta?\"))\n",
    "# r.print_response_stream()\n",
    "\n",
    "\n",
    "#\n",
    "#agent_chain = initialize_agent(\n",
    "#    tools, \n",
    "#    llm, \n",
    "#    agent=\"zero-shot-react-description\", \n",
    "#    memory=ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "#)\n",
    "\n",
    "#retriever = index.as_retriever(service_context=service_context)\n",
    "#query_engine = RetrieverQueryEngine.from_args(retriever, response_mode='no_text')  \n",
    "#response = query_engine.query(\"Give me a list of all the good and the bad things in the text.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
